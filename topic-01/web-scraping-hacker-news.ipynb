{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping Hacker News\n",
    "\n",
    "The website [Hacker News](https://news.ycombinator.com/) is a great source of information for developers and tech enthusiasts. It is a social news website focusing on computer science and entrepreneurship. It is run by Paul Graham's investment fund and startup incubator, Y Combinator.\n",
    "\n",
    "Let's scrape the website and extract the following information from the first page of the website:\n",
    "\n",
    "- Title of the post\n",
    "- Link to the post\n",
    "\n",
    "We will use the `requests` and `beautifulsoup` libraries to scrape the website.\n",
    "\n",
    "Let's start by importing the necessary libraries and defining a function to scrape the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_website(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using the `requests` library to download the HTML content of the website and the `beautifulsoup` library to parse the HTML content and extract the information we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://news.ycombinator.com/news\"\n",
    "\n",
    "soup = scrape_website(url)\n",
    "\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've scraped the landing page, but we need to extract the link information for the posts. Let's inspect the HTML content of the website to find the class name of the link element.\n",
    "\n",
    "## Example HTML fragment\n",
    "\n",
    "<span class=\"titleline\"><a href=\"https://flyonui.com/\">Show HN: Flyon UI – Free Tailwind Components Library</a><span class=\"sitebit comhead\"> (<a href=\"from?site=flyonui.com\"><span class=\"sitestr\">flyonui.com</span></a>)</span></span>\n",
    "\n",
    "```html\n",
    "<span class=\"titleline\">\n",
    "\t<a href=\"https://flyonui.com/\">Show HN: Flyon UI – Free Tailwind Components Library</a>\n",
    "\t<span class=\"sitebit comhead\">\n",
    "\t\t(<a href=\"from?site=flyonui.com\"><span class=\"sitestr\">flyonui.com</span></a>)\n",
    "\t</span>\n",
    "</span>\n",
    "```\n",
    "\n",
    "The class `<a>` tag contains the link (`href` attribute) and the title of the post, but we need to find the parent `<span>` tag with the class `titleline` to extract the link and title.\n",
    "\n",
    "We'll start by finding all the `<span>` tags with the class `titleline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titlelines = soup.find_all('span', class_='titleline')\n",
    "\n",
    "titlelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now find the child `<a>` tag to extract the link and title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for titleline in titlelines:\n",
    "    link = titleline.find('a')\n",
    "    if link:\n",
    "        # Extract titles and links\n",
    "        data.append({\n",
    "            'title': link.text,\n",
    "            'link': link['href']\n",
    "        })\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make build a new Pandas `DataFrame` with the extracted information, which will make it easier to analyze and visualise the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "1. Modify the code to scrape a different website of your choice\n",
    "2. Extract additional information (e.g., article date, author, number of comments)\n",
    "3. Save the results to a CSV file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
