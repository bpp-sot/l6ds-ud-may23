{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analytics Fundamentals\n",
    "\n",
    "## Learning Objectives\n",
    "* Understand why text preprocessing is essential for analysis\n",
    "* Apply core text preprocessing techniques\n",
    "* Perform basic text analysis using Python\n",
    "* Gain hands-on experience with NLTK library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Why Text Preprocessing? \n",
    "\n",
    "### The Text Analysis Challenge\n",
    "\n",
    "Consider these challenges with raw text data:\n",
    "* Inconsistent formatting (uppercase/lowercase)\n",
    "* Punctuation and special characters\n",
    "* Common words that don't add meaning (\"the\", \"and\", \"is\")\n",
    "* Different forms of the same word (\"run\", \"running\", \"ran\")\n",
    "\n",
    "Let's see a practical example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text contains:\n",
      "- Mixed case: True\n",
      "- Punctuation: True\n",
      "- Special characters: True\n",
      "- Multiple lines: True\n"
     ]
    }
   ],
   "source": [
    "# Raw customer feedback example\n",
    "raw_feedback = \"\"\"\n",
    "GREAT product!!! I've been using it for 3 months now...\n",
    "The customer service team was really helpful :)\n",
    "Would definitely recommend to others!!!\n",
    "\"\"\"\n",
    "\n",
    "print(\"Raw text contains:\")\n",
    "print(\"- Mixed case:\", any(c.isupper() for c in raw_feedback))\n",
    "print(\"- Punctuation:\", any(c in \"!?.,\" for c in raw_feedback))\n",
    "print(\"- Special characters:\", \":)\" in raw_feedback)\n",
    "print(\"- Multiple lines:\", \"\\n\" in raw_feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Text Preprocessing Steps\n",
    "\n",
    "Let's set up our environment first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/vscode/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment ready!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "print(\"Environment ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Tokenisation\n",
    "\n",
    "Tokenisation breaks text into smaller pieces (tokens) - either sentences or words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences:\n",
      "1. The product exceeded my expectations!\n",
      "2. Customer service was excellent.\n",
      "3. Would recommend.\n",
      "\n",
      "Words: ['The', 'product', 'exceeded', 'my', 'expectations', '!', 'Customer', 'service', 'was', 'excellent', '.', 'Would', 'recommend', '.']\n",
      "Number of words: 14\n"
     ]
    }
   ],
   "source": [
    "# Example text\n",
    "text = \"The product exceeded my expectations! Customer service was excellent. Would recommend.\"\n",
    "\n",
    "# Sentence tokenisation\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"Sentences:\")\n",
    "for i, sent in enumerate(sentences, 1):\n",
    "    print(f\"{i}. {sent}\")\n",
    "\n",
    "# Word tokenisation\n",
    "words = word_tokenize(text)\n",
    "print(\"\\nWords:\", words)\n",
    "print(f\"Number of words: {len(words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Case Normalisation\n",
    "\n",
    "Converting text to consistent case helps with analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: The PRODUCT was AMAZING and I LOVED it!\n",
      "Normalized: the product was amazing and i loved it!\n",
      "\n",
      "Without normalization:\n",
      "\"PRODUCT\" == \"product\": False\n",
      "\n",
      "With normalization:\n",
      "\"PRODUCT\".lower() == \"product\".lower(): True\n"
     ]
    }
   ],
   "source": [
    "# Mixed case example\n",
    "mixed_text = \"The PRODUCT was AMAZING and I LOVED it!\"\n",
    "\n",
    "normalized = mixed_text.lower()\n",
    "print(\"Original:\", mixed_text)\n",
    "print(\"Normalized:\", normalized)\n",
    "\n",
    "# Why this matters\n",
    "print(\"\\nWithout normalization:\")\n",
    "print('\"PRODUCT\" == \"product\":', \"PRODUCT\" == \"product\")\n",
    "print('\\nWith normalization:')\n",
    "print('\"PRODUCT\".lower() == \"product\".lower():', \"PRODUCT\".lower() == \"product\".lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Stop Word Removal\n",
    "\n",
    "Stop words are common words that usually don't add meaning to our analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['the', 'product', 'is', 'very', 'good', 'and', 'i', 'would', 'recommend', 'it', 'to', 'others']\n",
      "After removing stop words: ['product', 'good', 'would', 'recommend', 'others']\n",
      "\n",
      "Reduced from 12 to 5 words\n"
     ]
    }
   ],
   "source": [
    "# Get English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Example text\n",
    "text = \"The product is very good and I would recommend it to others\"\n",
    "words = word_tokenize(text.lower())\n",
    "\n",
    "# Remove stop words\n",
    "filtered_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "print(\"Original words:\", words)\n",
    "print(\"After removing stop words:\", filtered_words)\n",
    "print(f\"\\nReduced from {len(words)} to {len(filtered_words)} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Stemming and Lemmatisation\n",
    "\n",
    "These techniques help find the root form of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original\tStem\t\tLemma\n",
      "----------------------------------------\n",
      "running\trun\t\trunning\n",
      "runs\trun\t\trun\n",
      "ran\tran\t\tran\n",
      "easily\teasili\t\teasily\n",
      "playing\tplay\t\tplaying\n",
      "better\tbetter\t\tbetter\n",
      "good\tgood\t\tgood\n"
     ]
    }
   ],
   "source": [
    "# Initialize stemmers and lemmatisers\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Example words\n",
    "words = [\"running\", \"runs\", \"ran\", \"easily\", \"playing\", \"better\", \"good\"]\n",
    "\n",
    "# Apply stemming and lemmatisation\n",
    "stems = [stemmer.stem(word) for word in words]\n",
    "lemmas = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "# Compare results\n",
    "print(\"Original\\tStem\\t\\tLemma\")\n",
    "print(\"-\" * 40)\n",
    "for original, stem, lemma in zip(words, stems, lemmas):\n",
    "    print(f\"{original}\\t{stem}\\t\\t{lemma}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the differnece between stemming and lemmatization?\n",
    "\n",
    "Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base, or root form — generally a written word form. Example: \"running\" -> \"run\". Stemming makes use of an algorithmic approach, which allows for the reduction of words to their root form. \n",
    "\n",
    "Lemmatisation is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form. Example: \"better\" -> \"good\". Lemmatisation makes use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.\n",
    "\n",
    "| Aspect | Stemming | Lemmatisation |\n",
    "|--------|----------|---------------|\n",
    "| **Definition** | Removes prefixes/suffixes using rules | Converts to dictionary base form |\n",
    "| **Output** | Can produce non-dictionary words | Always produces valid dictionary words |\n",
    "| **Speed** | Fast | Slower |\n",
    "| **Accuracy** | Lower | Higher |\n",
    "| **Example 1** | running → run | running → run |\n",
    "| **Example 2** | better → bet | better → good |\n",
    "| **Example 3** | easily → easili | easily → easy |\n",
    "| **Use Case** | Search engines, large-scale processing | Text analysis, NLP tasks |\n",
    "| **Resource Use** | Minimal (rule-based) | Higher (needs dictionary) |\n",
    "| **Context Aware** | No | Yes |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Basic Text Analysis (10 minutes)\n",
    "\n",
    "Let's apply what we've learned to analyze some text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis Results:\n",
      "Total words: 26\n",
      "Unique words: 20\n",
      "\n",
      "Most common words:\n",
      "- great: 3 times\n",
      "- data: 3 times\n",
      "- analysis: 2 times\n",
      "- visualization: 2 times\n",
      "- product: 1 times\n"
     ]
    }
   ],
   "source": [
    "def analyse_text(text):\n",
    "    \"\"\"Basic text analysis function\"\"\"\n",
    "    # Normalise the text\n",
    "    normalised_text = text.lower()\n",
    "\n",
    "    # Tokenise the normalised text\n",
    "    words = word_tokenize(normalised_text)\n",
    "\n",
    "    # Remove stop words and punctuation\n",
    "    words = [word for word in words\n",
    "             if word not in stop_words and word.isalnum()]\n",
    "\n",
    "    # Get word frequencies\n",
    "    word_freq = Counter(words)\n",
    "\n",
    "    return {\n",
    "        'total_words': len(words),\n",
    "        'unique_words': len(set(words)),\n",
    "        'most_common': word_freq.most_common(5)\n",
    "    }\n",
    "\n",
    "# Example text\n",
    "customer_reviews = \"\"\"\n",
    "Great product, really helpful for data analysis. The visualisation features are amazing.\n",
    "Easy to use and great support from the team. Would recommend for data science projects.\n",
    "Data visualisation has never been easier. Great tool for analysis.\n",
    "\"\"\"\n",
    "\n",
    "results = analyse_text(customer_reviews)\n",
    "\n",
    "print(\"Analysis Results:\")\n",
    "print(f\"Total words: {results['total_words']}\")\n",
    "print(f\"Unique words: {results['unique_words']}\")\n",
    "print(\"\\nMost common words:\")\n",
    "for word, count in results['most_common']:\n",
    "    print(f\"- {word}: {count} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise (10 minutes)\n",
    "\n",
    "Try analysing some text from your own organisation (e.g., customer feedback, product documentation, or internal communications). Use the template below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Text Analysis:\n",
      "Total words: 2\n",
      "Unique words: 2\n",
      "\n",
      "Most common words:\n",
      "- replace: 1 times\n",
      "- text: 1 times\n"
     ]
    }
   ],
   "source": [
    "# Your text here\n",
    "your_text = \"\"\"\n",
    "Replace this with your own text\n",
    "\"\"\"\n",
    "\n",
    "# Analyze it\n",
    "your_results = analyse_text(your_text)\n",
    "\n",
    "# Print results\n",
    "print(\"Your Text Analysis:\")\n",
    "print(f\"Total words: {your_results['total_words']}\")\n",
    "print(f\"Unique words: {your_results['unique_words']}\")\n",
    "print(\"\\nMost common words:\")\n",
    "for word, count in your_results['most_common']:\n",
    "    print(f\"- {word}: {count} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In this session, we've covered:\n",
    "1. Why text preprocessing is necessary\n",
    "2. Key preprocessing techniques:\n",
    "   - Tokenisation\n",
    "   - Case normalisation\n",
    "   - Stop word removal\n",
    "   - Stemming and lemmatisation\n",
    "3. Basic text analysis methods\n",
    "\n",
    "### Next Steps\n",
    "- Explore more advanced text analysis techniques\n",
    "- Learn about sentiment analysis\n",
    "- Practice with real-world text data\n",
    "\n",
    "### Additional Resources\n",
    "- [NLTK Documentation](https://www.nltk.org/)\n",
    "- [Text Analytics with Python](https://www.nltk.org/book/)\n",
    "- [Practical Text Mining with Python](https://realpython.com/nltk-nlp-python/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
