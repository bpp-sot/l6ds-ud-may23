{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to LLMs and Prompt Engineering\n",
    "## Working with OpenAI's API in Python\n",
    "\n",
    "In this notebook, we'll learn how to:\n",
    "* Work with the OpenAI API\n",
    "* Write effective prompts\n",
    "* Generate structured outputs from unstructured text\n",
    "* Handle common errors and edge cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Installation\n",
    "First, we need to install and import required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up API Access\n",
    "\n",
    "We need to set up our API key to authenticate with OpenAI's services. Never share your API key or commit it to version control!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API key\n",
    "# Best practice is to use environment variables\n",
    "openai = OpenAI(\n",
    "\tapi_key=os.environ.get(\"OPENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic API Call\n",
    "\n",
    "Let's create a helper function to make API calls easier and more consistent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0  # this makes the output more deterministic\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Test the function\n",
    "prompt = \"What is a large language model?\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Zero-shot Prompting\n",
    "\n",
    "Zero-shot prompting means asking the model to perform a task without any examples. Let's try a sentiment analysis task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "The customer service was terrible. I waited for 45 minutes and nobody helped me.\n",
    "The store was messy and disorganized. Would not recommend.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Analyze the sentiment of the following text. Respond with either 'positive',\n",
    "'negative', or 'neutral':\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(f\"Sentiment: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Few-shot Prompting\n",
    "\n",
    "Few-shot prompting improves performance by providing examples of the desired behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Classify these reviews as positive or negative:\n",
    "\n",
    "Review: \"This restaurant was amazing! Great food!\"\n",
    "Classification: positive\n",
    "\n",
    "Review: \"Horrible experience, never coming back.\"\n",
    "Classification: negative\n",
    "\n",
    "Review: \"The pasta was cold and service was slow.\"\n",
    "Classification: negative\n",
    "\n",
    "Review: {text}\n",
    "Classification:\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(f\"Classification: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Chain-of-thought Prompting\n",
    "\n",
    "Chain-of-thought prompting helps the model break down complex reasoning tasks into steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The new iPhone costs $999 and comes with a charger worth $49.\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Follow these steps to analyze the text:\n",
    "1. Identify all monetary values\n",
    "2. Sum the total cost\n",
    "3. Convert the sum to euros using a rate of 1 USD = 0.85 EUR\n",
    "4. Format your response as JSON\n",
    "\n",
    "Text: {text}\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Structured Output Generation\n",
    "\n",
    "We can ask the model to return data in specific formats like JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_review = \"\"\"\n",
    "I bought the XPS 13 laptop last month. The screen is incredibly sharp and colorful.\n",
    "Battery life is about 8 hours. The keyboard feels great but the trackpad is a bit small.\n",
    "It's expensive at $1299 but worth it for the build quality.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Extract product information from the review and return it as JSON with these fields:\n",
    "- product_name\n",
    "- price\n",
    "- pros (as an array)\n",
    "- cons (as an array)\n",
    "\n",
    "Review: {product_review}\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(json.dumps(json.loads(response), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Exercise\n",
    "\n",
    "Now it's your turn! Try writing prompts to extract information from this text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "During the company meeting on March 15, 2024, we discussed the Q4 results from 2023.\n",
    "The marketing team launched a new campaign on April 1st that increased sales by 25%.\n",
    "We plan to expand into European markets by December 2024.\n",
    "\"\"\"\n",
    "\n",
    "# Write your prompt to extract dates and events\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Handling\n",
    "\n",
    "It's important to handle potential API errors gracefully:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    try:\n",
    "        response = get_completion(prompt, model)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# Test error handling\n",
    "response = safe_get_completion(\"\", model=\"nonexistent-model\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
