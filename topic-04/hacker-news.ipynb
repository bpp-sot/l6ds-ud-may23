{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Working with the Hacker News API\n",
				"\n",
				"This notebook demonstrates how to work with the Hacker News API, which is a RESTful API that provides access to stories, comments, jobs, and more from Hacker News. We'll apply the HTTP and API concepts we've learned to extract, analyse, and visualise data from Hacker News."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"%pip install --quiet matplotlib pandas requests IPython seaborn networkx wordcloud scikit-learn textblob"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Understanding the Hacker News API\n",
				"\n",
				"The Hacker News API is a simple, RESTful API that provides access to Hacker News content. The base URL for the API is `https://hacker-news.firebaseio.com/v0/`.\n",
				"\n",
				"Here are some of the key endpoints we'll be using:\n",
				"\n",
				"- `/topstories.json` - Returns the IDs of the top stories on Hacker News\n",
				"- `/newstories.json` - Returns the IDs of the newest stories on Hacker News\n",
				"- `/beststories.json` - Returns the IDs of the best stories on Hacker News\n",
				"- `/item/{id}.json` - Returns details about an item (story, comment, job, etc.)\n",
				"- `/user/{id}.json` - Returns details about a user\n",
				"\n",
				"Let's set up our HTTP debugger from the previous notebook:"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"import requests\n",
				"from requests import Request, Response, Session\n",
				"from urllib.parse import urlparse, parse_qs, urlencode, urlunparse\n",
				"import json\n",
				"import pandas as pd\n",
				"import matplotlib.pyplot as plt\n",
				"import time\n",
				"from datetime import datetime\n",
				"import seaborn as sns\n",
				"\n",
				"# Set up HTTP request/response debugger\n",
				"def print_request(request: Request):\n",
				"    \"\"\"\n",
				"    Print the details of an HTTP request.\n",
				"    \"\"\"\n",
				"    url = urlparse(request.url)\n",
				"    uri = \"?\".join([url.path, url.query]) if url.query else url.path\n",
				"\n",
				"    print(f\"\\n--> HTTP Request to {url.netloc}\")\n",
				"    print(f\"REQUEST: {request.method} {uri}\")\n",
				"    print(f\"HEADERS:\")\n",
				"\n",
				"    for key, value in request.headers.items():\n",
				"        print(f\"  {key}: {value}\")\n",
				"\n",
				"    if request.body:\n",
				"        print(f\"BODY: {request.body[:100]}...\" if len(request.body) > 100 else f\"BODY: {request.body}\")\n",
				"\n",
				"def print_response(response: Response):\n",
				"    \"\"\"\n",
				"    Print the details of an HTTP response.\n",
				"    \"\"\"\n",
				"    url = urlparse(response.url)\n",
				"\n",
				"    print(f\"\\n<-- HTTP Response from {url.netloc}\")\n",
				"    print(f\"RESPONSE: {response.status_code} {response.reason}\")\n",
				"    print(f\"HEADERS:\")\n",
				"\n",
				"    for key, value in response.headers.items():\n",
				"        print(f\"  {key}: {value}\")\n",
				"\n",
				"    if response.text:\n",
				"        print(f\"BODY: {response.text[:100]}...\" if len(response.text) > 100 else f\"BODY: {response.text}\")\n",
				"\n",
				"def http_logger(response, *args, **kwargs):\n",
				"    \"\"\"\n",
				"    Log the details of an HTTP request and response.\n",
				"    \"\"\"\n",
				"    print_request(response.request)\n",
				"    print_response(response)\n",
				"\n",
				"# Create a session to track request/response details\n",
				"session = Session()\n",
				"session.hooks[\"response\"] = [http_logger]\n",
				"\n",
				"# Base URL for the Hacker News API\n",
				"base_url = \"https://hacker-news.firebaseio.com/v0/\""
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Getting the Top Stories\n",
				"\n",
				"Let's start by getting the IDs of the top stories on Hacker News. This endpoint returns an array of story IDs."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Get the top stories\n",
				"top_stories_url = f\"{base_url}topstories.json\"\n",
				"response = session.get(top_stories_url)\n",
				"\n",
				"# Check if the request was successful\n",
				"if response.status_code == 200:\n",
				"    # Parse the JSON response\n",
				"    top_story_ids = response.json()\n",
				"    print(f\"Retrieved {len(top_story_ids)} top story IDs\")\n",
				"    print(f\"First 10 story IDs: {top_story_ids[:10]}\")\n",
				"else:\n",
				"    print(f\"Error: {response.status_code}\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Fetching Story Details\n",
				"\n",
				"Now that we have the IDs of the top stories, let's fetch the details of the first 10 stories. Each story has details like title, URL, score, author, etc.\n",
				"\n",
				"We'll also implement a rate-limiting mechanism to be respectful of the API."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"def fetch_item(item_id, session=None):\n",
				"    \"\"\"\n",
				"    Fetch the details of an item (story, comment, job, etc.) from the Hacker News API.\n",
				"\n",
				"    :param item_id: The ID of the item to fetch\n",
				"    :param session: The session to use for the request (optional)\n",
				"    :return: The item details as a dictionary\n",
				"    \"\"\"\n",
				"    # Use the provided session or create a new one\n",
				"    s = session if session else requests.Session()\n",
				"\n",
				"    # Construct the URL for the item\n",
				"    item_url = f\"{base_url}item/{item_id}.json\"\n",
				"\n",
				"    # Make the request\n",
				"    response = s.get(item_url)\n",
				"\n",
				"    # Check if the request was successful\n",
				"    if response.status_code == 200:\n",
				"        return response.json()\n",
				"    else:\n",
				"        print(f\"Error fetching item {item_id}: {response.status_code}\")\n",
				"        return None\n",
				"\n",
				"# Fetch the details of the first 10 top stories with rate limiting\n",
				"top_stories = []\n",
				"for i, story_id in enumerate(top_story_ids[:10]):\n",
				"    # Add a small delay to be respectful of the API\n",
				"    if i > 0:\n",
				"        time.sleep(0.1)  # 100ms delay between requests\n",
				"\n",
				"    story = fetch_item(story_id, session)\n",
				"    if story:\n",
				"        top_stories.append(story)\n",
				"\n",
				"print(f\"Retrieved details for {len(top_stories)} top stories\")\n",
				"\n",
				"# Display the first story as an example\n",
				"if top_stories:\n",
				"    print(json.dumps(top_stories[0], indent=2))"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Transforming the Data\n",
				"\n",
				"Let's transform the story data into a pandas DataFrame for easier analysis. We'll extract relevant fields like title, URL, score, author, etc."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Extract relevant fields from each story and create a DataFrame\n",
				"story_data = []\n",
				"for story in top_stories:\n",
				"    # Skip stories without a title (deleted or invalid)\n",
				"    if 'title' not in story:\n",
				"        continue\n",
				"\n",
				"    # Extract relevant fields\n",
				"    story_dict = {\n",
				"        'id': story.get('id'),\n",
				"        'title': story.get('title'),\n",
				"        'url': story.get('url', ''),  # Some stories don't have a URL (text posts)\n",
				"        'score': story.get('score', 0),\n",
				"        'author': story.get('by', ''),\n",
				"        'time': story.get('time', 0),  # Unix timestamp\n",
				"        'num_comments': story.get('descendants', 0),\n",
				"        'type': story.get('type', ''),\n",
				"        'has_url': 'url' in story  # Flag to indicate if the story has a URL\n",
				"    }\n",
				"\n",
				"    # Convert Unix timestamp to datetime\n",
				"    if story_dict['time']:\n",
				"        story_dict['datetime'] = datetime.fromtimestamp(story_dict['time'])\n",
				"\n",
				"    # Extract domain from URL if available\n",
				"    if story_dict['url']:\n",
				"        try:\n",
				"            parsed_url = urlparse(story_dict['url'])\n",
				"            story_dict['domain'] = parsed_url.netloc\n",
				"        except:\n",
				"            story_dict['domain'] = ''\n",
				"    else:\n",
				"        story_dict['domain'] = ''\n",
				"\n",
				"    story_data.append(story_dict)\n",
				"\n",
				"# Create DataFrame\n",
				"df_stories = pd.DataFrame(story_data)\n",
				"\n",
				"# Display the DataFrame\n",
				"df_stories"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Let's scale up: Getting 30 Top Stories\n",
				"\n",
				"Now let's fetch more stories for better analysis. We'll fetch the top 30 stories and add them to our DataFrame."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Clear our existing data and fetch more stories\n",
				"top_stories = []\n",
				"for i, story_id in enumerate(top_story_ids[:30]):\n",
				"    # Add a small delay to be respectful of the API\n",
				"    if i > 0:\n",
				"        time.sleep(0.1)  # 100ms delay between requests\n",
				"\n",
				"    story = fetch_item(story_id)\n",
				"    if story:\n",
				"        top_stories.append(story)\n",
				"\n",
				"print(f\"Retrieved details for {len(top_stories)} top stories\")\n",
				"\n",
				"# Extract relevant fields and create DataFrame as before\n",
				"story_data = []\n",
				"for story in top_stories:\n",
				"    if 'title' not in story:\n",
				"        continue\n",
				"\n",
				"    story_dict = {\n",
				"        'id': story.get('id'),\n",
				"        'title': story.get('title'),\n",
				"        'url': story.get('url', ''),\n",
				"        'score': story.get('score', 0),\n",
				"        'author': story.get('by', ''),\n",
				"        'time': story.get('time', 0),\n",
				"        'num_comments': story.get('descendants', 0),\n",
				"        'type': story.get('type', ''),\n",
				"        'has_url': 'url' in story\n",
				"    }\n",
				"\n",
				"    if story_dict['time']:\n",
				"        story_dict['datetime'] = datetime.fromtimestamp(story_dict['time'])\n",
				"\n",
				"    if story_dict['url']:\n",
				"        try:\n",
				"            parsed_url = urlparse(story_dict['url'])\n",
				"            story_dict['domain'] = parsed_url.netloc\n",
				"        except:\n",
				"            story_dict['domain'] = ''\n",
				"    else:\n",
				"        story_dict['domain'] = ''\n",
				"\n",
				"    story_data.append(story_dict)\n",
				"\n",
				"df_stories = pd.DataFrame(story_data)\n",
				"\n",
				"# Display the number of stories and the first few rows\n",
				"print(f\"DataFrame contains {len(df_stories)} stories\")\n",
				"df_stories.head()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Data Analysis and Visualisation\n",
				"\n",
				"Now that we have a reasonable amount of data, let's analyse and visualise it."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Basic statistics\n",
				"print(\"Basic statistics for scores:\")\n",
				"print(df_stories['score'].describe())\n",
				"\n",
				"print(\"\\nBasic statistics for number of comments:\")\n",
				"print(df_stories['num_comments'].describe())"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Correlation between score and number of comments\n",
				"correlation = df_stories['score'].corr(df_stories['num_comments'])\n",
				"print(f\"Correlation between score and number of comments: {correlation:.2f}\")\n",
				"\n",
				"# Scatter plot of score vs. number of comments\n",
				"plt.figure(figsize=(10, 6))\n",
				"sns.scatterplot(x='score', y='num_comments', data=df_stories)\n",
				"plt.title('Score vs. Number of Comments')\n",
				"plt.xlabel('Score')\n",
				"plt.ylabel('Number of Comments')\n",
				"plt.show()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Top domains\n",
				"domain_counts = df_stories['domain'].value_counts().head(10)\n",
				"print(\"Top domains:\")\n",
				"print(domain_counts)\n",
				"\n",
				"# Bar chart of top domains\n",
				"plt.figure(figsize=(12, 6))\n",
				"sns.barplot(x=domain_counts.index, y=domain_counts.values)\n",
				"plt.title('Top 10 Domains in Hacker News Top Stories')\n",
				"plt.xlabel('Domain')\n",
				"plt.ylabel('Count')\n",
				"plt.xticks(rotation=45, ha='right')\n",
				"plt.tight_layout()\n",
				"plt.show()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Distribution of scores\n",
				"plt.figure(figsize=(10, 6))\n",
				"sns.histplot(df_stories['score'], bins=20, kde=True)\n",
				"plt.title('Distribution of Story Scores')\n",
				"plt.xlabel('Score')\n",
				"plt.ylabel('Frequency')\n",
				"plt.show()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Posts by hour of day\n",
				"df_stories['hour'] = df_stories['datetime'].dt.hour\n",
				"hour_counts = df_stories['hour'].value_counts().sort_index()\n",
				"\n",
				"plt.figure(figsize=(12, 6))\n",
				"sns.barplot(x=hour_counts.index, y=hour_counts.values)\n",
				"plt.title('Distribution of Top Stories by Hour of Day (UTC)')\n",
				"plt.xlabel('Hour of Day (UTC)')\n",
				"plt.ylabel('Number of Stories')\n",
				"plt.xticks(range(0, 24))\n",
				"plt.show()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Getting and Analysing Comments\n",
				"\n",
				"Let's dive deeper by fetching and analysing the comments for the top story."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Get the top story ID\n",
				"top_story_id = df_stories.iloc[0]['id']\n",
				"top_story = fetch_item(top_story_id)\n",
				"\n",
				"print(f\"Analysing comments for the top story: {top_story['title']}\")\n",
				"\n",
				"# Recursive function to fetch all comments and replies\n",
				"def fetch_comments_tree(item_id, level=0, max_level=2):\n",
				"    \"\"\"\n",
				"    Recursively fetch comments and their replies up to a maximum depth.\n",
				"\n",
				"    :param item_id: The ID of the item to fetch\n",
				"    :param level: The current recursion level\n",
				"    :param max_level: The maximum depth to recurse\n",
				"    :return: A list of comment dictionaries with metadata\n",
				"    \"\"\"\n",
				"    # Stop recursion if we've reached the maximum level\n",
				"    if level > max_level:\n",
				"        return []\n",
				"\n",
				"    # Fetch the item\n",
				"    item = fetch_item(item_id)\n",
				"    if not item or 'deleted' in item or 'dead' in item:\n",
				"        return []\n",
				"\n",
				"    # Add level information to the item\n",
				"    item['level'] = level\n",
				"\n",
				"    # Base case: no kids/replies\n",
				"    if 'kids' not in item:\n",
				"        return [item]\n",
				"\n",
				"    # Recursive case: fetch replies\n",
				"    comments = [item]\n",
				"    for kid_id in item.get('kids', []):\n",
				"        # Add a small delay to be respectful of the API\n",
				"        time.sleep(0.1)\n",
				"        comments.extend(fetch_comments_tree(kid_id, level + 1, max_level))\n",
				"\n",
				"    return comments\n",
				"\n",
				"# Fetch comments for the top story (limited to depth 1 for demonstration)\n",
				"if 'kids' in top_story:\n",
				"    print(f\"Fetching comments for story {top_story_id} (this might take a while)...\")\n",
				"    comments = []\n",
				"    for i, kid_id in enumerate(top_story.get('kids', [])[:5]):  # Limit to first 5 comments\n",
				"        if i > 0:\n",
				"            time.sleep(0.1)  # Be nice to the API\n",
				"        comments.extend(fetch_comments_tree(kid_id, level=0, max_level=1))\n",
				"\n",
				"    print(f\"Retrieved {len(comments)} comments and replies\")\n",
				"\n",
				"    # Transform comment data into a DataFrame\n",
				"    comment_data = []\n",
				"    for comment in comments:\n",
				"        if 'text' not in comment:\n",
				"            continue\n",
				"\n",
				"        comment_dict = {\n",
				"            'id': comment.get('id'),\n",
				"            'author': comment.get('by', ''),\n",
				"            'text': comment.get('text', ''),\n",
				"            'time': comment.get('time', 0),\n",
				"            'level': comment.get('level', 0),\n",
				"            'parent': comment.get('parent'),\n",
				"            'has_replies': 'kids' in comment,\n",
				"            'num_replies': len(comment.get('kids', []))\n",
				"        }\n",
				"\n",
				"        # Convert timestamp to datetime\n",
				"        if comment_dict['time']:\n",
				"            comment_dict['datetime'] = datetime.fromtimestamp(comment_dict['time'])\n",
				"\n",
				"        comment_data.append(comment_dict)\n",
				"\n",
				"    df_comments = pd.DataFrame(comment_data)\n",
				"\n",
				"    # Display the first few comments\n",
				"    print(\"\\nSample of comments:\")\n",
				"    for i, row in df_comments.head(3).iterrows():\n",
				"        print(f\"\\nLevel {row['level']} comment by {row['author']}:\")\n",
				"        # Truncate long comments for display\n",
				"        text = row['text'][:100] + '...' if len(row['text']) > 100 else row['text']\n",
				"        print(text)\n",
				"else:\n",
				"    print(\"No comments found for this story.\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Analysing User Activity\n",
				"\n",
				"Let's fetch details about some of the most active users in our dataset."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"def fetch_user(username):\n",
				"    \"\"\"\n",
				"    Fetch user details from the Hacker News API.\n",
				"\n",
				"    :param username: The username to fetch\n",
				"    :return: The user details as a dictionary\n",
				"    \"\"\"\n",
				"    user_url = f\"{base_url}user/{username}.json\"\n",
				"    response = requests.get(user_url)\n",
				"\n",
				"    if response.status_code == 200:\n",
				"        return response.json()\n",
				"    else:\n",
				"        print(f\"Error fetching user {username}: {response.status_code}\")\n",
				"        return None\n",
				"\n",
				"# Find the top 5 most frequent authors in our stories\n",
				"top_authors = df_stories['author'].value_counts().head(5).index.tolist()\n",
				"\n",
				"print(f\"Top 5 most frequent authors in our dataset: {top_authors}\")\n",
				"\n",
				"# Fetch details for these authors\n",
				"users = []\n",
				"for author in top_authors:\n",
				"    time.sleep(0.1)  # Be nice to the API\n",
				"    user = fetch_user(author)\n",
				"    if user:\n",
				"        users.append(user)\n",
				"\n",
				"# Create DataFrame with user details\n",
				"user_data = []\n",
				"for user in users:\n",
				"    user_dict = {\n",
				"        'id': user.get('id'),\n",
				"        'karma': user.get('karma', 0),\n",
				"        'created': user.get('created', 0),\n",
				"        'submitted_count': len(user.get('submitted', [])),\n",
				"        'about': user.get('about', '')\n",
				"    }\n",
				"\n",
				"    # Convert timestamp to datetime\n",
				"    if user_dict['created']:\n",
				"        user_dict['created_date'] = datetime.fromtimestamp(user_dict['created'])\n",
				"        user_dict['account_age_days'] = (datetime.now() - user_dict['created_date']).days\n",
				"\n",
				"    user_data.append(user_dict)\n",
				"\n",
				"df_users = pd.DataFrame(user_data)\n",
				"\n",
				"# Display user information\n",
				"df_users[['id', 'karma', 'created_date', 'account_age_days', 'submitted_count']]"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Visualise user karma vs submitted count\n",
				"plt.figure(figsize=(10, 6))\n",
				"sns.scatterplot(x='karma', y='submitted_count', data=df_users, s=100)\n",
				"for i, row in df_users.iterrows():\n",
				"    plt.annotate(row['id'], (row['karma'], row['submitted_count']),\n",
				"                 xytext=(5, 5), textcoords='offset points')\n",
				"plt.title('User Karma vs Number of Submissions')\n",
				"plt.xlabel('Karma')\n",
				"plt.ylabel('Number of Submissions')\n",
				"plt.show()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Comparing Different Story Types\n",
				"\n",
				"Let's compare the top, new, and best stories on Hacker News."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Function to fetch IDs for different story types\n",
				"def get_story_ids(story_type):\n",
				"    \"\"\"\n",
				"    Fetch story IDs for a specific type (top, new, best).\n",
				"\n",
				"    :param story_type: The type of stories to fetch ('top', 'new', or 'best')\n",
				"    :return: A list of story IDs\n",
				"    \"\"\"\n",
				"    url = f\"{base_url}{story_type}stories.json\"\n",
				"    response = requests.get(url)\n",
				"\n",
				"    if response.status_code == 200:\n",
				"        return response.json()\n",
				"    else:\n",
				"        print(f\"Error fetching {story_type} stories: {response.status_code}\")\n",
				"        return []\n",
				"\n",
				"# Fetch IDs for new and best stories\n",
				"new_story_ids = get_story_ids('new')\n",
				"best_story_ids = get_story_ids('best')\n",
				"\n",
				"print(f\"Retrieved {len(new_story_ids)} new story IDs\")\n",
				"print(f\"Retrieved {len(best_story_ids)} best story IDs\")\n",
				"\n",
				"# Fetch details for the first 10 stories of each type\n",
				"fetch_story_details = lambda ids: [fetch_item(id) for id in ids[:10]]\n",
				"\n",
				"# We already have top stories, so just fetch new and best\n",
				"new_stories = []\n",
				"best_stories = []\n",
				"\n",
				"print(\"Fetching new stories...\")\n",
				"for i, story_id in enumerate(new_story_ids[:10]):\n",
				"    if i > 0:\n",
				"        time.sleep(0.1)  # Be nice to the API\n",
				"    story = fetch_item(story_id)\n",
				"    if story:\n",
				"        new_stories.append(story)\n",
				"\n",
				"print(\"Fetching best stories...\")\n",
				"for i, story_id in enumerate(best_story_ids[:10]):\n",
				"    if i > 0:\n",
				"        time.sleep(0.1)  # Be nice to the API\n",
				"    story = fetch_item(story_id)\n",
				"    if story:\n",
				"        best_stories.append(story)\n",
				"\n",
				"print(f\"Retrieved {len(new_stories)} new stories\")\n",
				"print(f\"Retrieved {len(best_stories)} best stories\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Function to convert story data to DataFrame\n",
				"def stories_to_dataframe(stories, story_type):\n",
				"    \"\"\"\n",
				"    Convert story data to a DataFrame.\n",
				"\n",
				"    :param stories: List of story dictionaries\n",
				"    :param story_type: Type of story ('top', 'new', or 'best')\n",
				"    :return: DataFrame with story data\n",
				"    \"\"\"\n",
				"    story_data = []\n",
				"\n",
				"    for story in stories:\n",
				"        if 'title' not in story:\n",
				"            continue\n",
				"\n",
				"        story_dict = {\n",
				"            'id': story.get('id'),\n",
				"            'title': story.get('title'),\n",
				"            'score': story.get('score', 0),\n",
				"            'author': story.get('by', ''),\n",
				"            'time': story.get('time', 0),\n",
				"            'num_comments': story.get('descendants', 0),\n",
				"            'story_type': story_type  # Add the story type as a column\n",
				"        }\n",
				"\n",
				"        # Convert timestamp to datetime\n",
				"        if story_dict['time']:\n",
				"            story_dict['datetime'] = datetime.fromtimestamp(story_dict['time'])\n",
				"            story_dict['age_hours'] = (datetime.now() - story_dict['datetime']).total_seconds() / 3600\n",
				"\n",
				"        story_data.append(story_dict)\n",
				"\n",
				"    return pd.DataFrame(story_data)\n",
				"\n",
				"# Create DataFrames for each story type\n",
				"df_top = stories_to_dataframe(top_stories[:10], 'top')\n",
				"df_new = stories_to_dataframe(new_stories, 'new')\n",
				"df_best = stories_to_dataframe(best_stories, 'best')\n",
				"\n",
				"# Combine into a single DataFrame\n",
				"df_all_stories = pd.concat([df_top, df_new, df_best], ignore_index=True)\n",
				"\n",
				"# Display the first few rows\n",
				"df_all_stories.head()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Compare scores and comments across story types\n",
				"# Calculate average score and number of comments for each story type\n",
				"story_type_stats = df_all_stories.groupby('story_type').agg({\n",
				"    'score': 'mean',\n",
				"    'num_comments': 'mean',\n",
				"    'age_hours': 'mean'\n",
				"}).reset_index()\n",
				"\n",
				"print(\"Average score, comments, and age by story type:\")\n",
				"print(story_type_stats)\n",
				"\n",
				"# Create a bar chart to compare scores\n",
				"plt.figure(figsize=(12, 6))\n",
				"plt.subplot(1, 2, 1)\n",
				"sns.barplot(x='story_type', y='score', data=df_all_stories)\n",
				"plt.title('Distribution of Scores by Story Type')\n",
				"plt.xlabel('Story Type')\n",
				"plt.ylabel('Score')\n",
				"\n",
				"# Create a bar chart to compare number of comments\n",
				"plt.subplot(1, 2, 2)\n",
				"sns.barplot(x='story_type', y='num_comments', data=df_all_stories)\n",
				"plt.title('Distribution of Comments by Story Type')\n",
				"plt.xlabel('Story Type')\n",
				"plt.ylabel('Number of Comments')\n",
				"\n",
				"plt.tight_layout()\n",
				"plt.show()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Analysing Submission Times\n",
				"\n",
				"Let's analyse when stories are submitted to Hacker News and if there's any relationship between submission time and popularity."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Extract hour and day of week from datetime\n",
				"df_all_stories['hour'] = df_all_stories['datetime'].dt.hour\n",
				"df_all_stories['day_of_week'] = df_all_stories['datetime'].dt.day_name()\n",
				"\n",
				"# Calculate average score and comments by hour of day\n",
				"hour_stats = df_all_stories.groupby('hour').agg({\n",
				"    'score': 'mean',\n",
				"    'num_comments': 'mean',\n",
				"    'id': 'count'  # Count of stories\n",
				"}).rename(columns={'id': 'count'}).reset_index()\n",
				"\n",
				"# Plot average score by hour of day\n",
				"plt.figure(figsize=(12, 6))\n",
				"plt.subplot(1, 2, 1)\n",
				"sns.lineplot(x='hour', y='score', data=hour_stats, marker='o')\n",
				"plt.title('Average Score by Hour of Day (UTC)')\n",
				"plt.xlabel('Hour of Day')\n",
				"plt.ylabel('Average Score')\n",
				"plt.xticks(range(0, 24, 2))\n",
				"\n",
				"# Plot average comments by hour of day\n",
				"plt.subplot(1, 2, 2)\n",
				"sns.lineplot(x='hour', y='num_comments', data=hour_stats, marker='o')\n",
				"plt.title('Average Comments by Hour of Day (UTC)')\n",
				"plt.xlabel('Hour of Day')\n",
				"plt.ylabel('Average Number of Comments')\n",
				"plt.xticks(range(0, 24, 2))\n",
				"\n",
				"plt.tight_layout()\n",
				"plt.show()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Ask HN vs. Show HN vs. Regular Posts\n",
				"\n",
				"Hacker News has special post types like \"Ask HN\" and \"Show HN\". Let's analyse how these different post types perform."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Identify post types based on title\n",
				"def get_post_type(title):\n",
				"    if title.startswith('Ask HN:'):\n",
				"        return 'Ask HN'\n",
				"    elif title.startswith('Show HN:'):\n",
				"        return 'Show HN'\n",
				"    elif title.startswith('Tell HN:'):\n",
				"        return 'Tell HN'\n",
				"    else:\n",
				"        return 'Regular'\n",
				"\n",
				"# Add post type column\n",
				"df_all_stories['post_type'] = df_all_stories['title'].apply(get_post_type)\n",
				"\n",
				"# Display counts by post type\n",
				"post_type_counts = df_all_stories['post_type'].value_counts()\n",
				"print(\"Counts by post type:\")\n",
				"print(post_type_counts)\n",
				"\n",
				"# Calculate statistics by post type\n",
				"post_type_stats = df_all_stories.groupby('post_type').agg({\n",
				"    'score': ['mean', 'median'],\n",
				"    'num_comments': ['mean', 'median'],\n",
				"    'id': 'count'\n",
				"}).reset_index()\n",
				"\n",
				"# Flatten the multi-level columns\n",
				"post_type_stats.columns = ['_'.join(col).strip('_') for col in post_type_stats.columns.values]\n",
				"\n",
				"print(\"\\nStatistics by post type:\")\n",
				"print(post_type_stats)\n",
				"\n",
				"# Visualise post types by score and comments\n",
				"plt.figure(figsize=(12, 8))\n",
				"\n",
				"plt.subplot(2, 1, 1)\n",
				"sns.boxplot(x='post_type', y='score', data=df_all_stories)\n",
				"plt.title('Distribution of Scores by Post Type')\n",
				"plt.xlabel('Post Type')\n",
				"plt.ylabel('Score')\n",
				"\n",
				"plt.subplot(2, 1, 2)\n",
				"sns.boxplot(x='post_type', y='num_comments', data=df_all_stories)\n",
				"plt.title('Distribution of Comments by Post Type')\n",
				"plt.xlabel('Post Type')\n",
				"plt.ylabel('Number of Comments')\n",
				"\n",
				"plt.tight_layout()\n",
				"plt.show()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Sentiment Analysis of Titles\n",
				"\n",
				"Let's analyse the sentiment of story titles to see if there's any relationship between sentiment and popularity."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# For a simple sentiment analysis, we can use a list of positive and negative words\n",
				"# This is a very basic approach - for a real analysis, you might use NLTK or TextBlob\n",
				"from textblob import TextBlob\n",
				"\n",
				"# Function to get sentiment polarity using TextBlob\n",
				"def get_sentiment(text):\n",
				"    blob = TextBlob(text)\n",
				"    return blob.sentiment.polarity  # Returns a value between -1 (negative) and 1 (positive)\n",
				"\n",
				"# Calculate sentiment for each title\n",
				"df_all_stories['sentiment'] = df_all_stories['title'].apply(get_sentiment)\n",
				"\n",
				"# Display average sentiment by post type\n",
				"print(\"Average sentiment by post type:\")\n",
				"print(df_all_stories.groupby('post_type')['sentiment'].mean())\n",
				"\n",
				"# Display sentiment distribution\n",
				"plt.figure(figsize=(10, 6))\n",
				"sns.histplot(df_all_stories['sentiment'], bins=20, kde=True)\n",
				"plt.title('Distribution of Title Sentiment')\n",
				"plt.xlabel('Sentiment Polarity (-1 to 1)')\n",
				"plt.ylabel('Frequency')\n",
				"plt.axvline(x=0, color='r', linestyle='--')  # Add line at neutral sentiment\n",
				"plt.show()\n",
				"\n",
				"# Check if there's a correlation between sentiment and score/comments\n",
				"sentiment_score_corr = df_all_stories['sentiment'].corr(df_all_stories['score'])\n",
				"sentiment_comments_corr = df_all_stories['sentiment'].corr(df_all_stories['num_comments'])\n",
				"\n",
				"print(f\"Correlation between sentiment and score: {sentiment_score_corr:.3f}\")\n",
				"print(f\"Correlation between sentiment and comments: {sentiment_comments_corr:.3f}\")\n",
				"\n",
				"# Scatter plot of sentiment vs. score\n",
				"plt.figure(figsize=(12, 6))\n",
				"plt.subplot(1, 2, 1)\n",
				"sns.scatterplot(x='sentiment', y='score', data=df_all_stories)\n",
				"plt.title('Sentiment vs. Score')\n",
				"plt.xlabel('Sentiment Polarity')\n",
				"plt.ylabel('Score')\n",
				"\n",
				"# Scatter plot of sentiment vs. comments\n",
				"plt.subplot(1, 2, 2)\n",
				"sns.scatterplot(x='sentiment', y='num_comments', data=df_all_stories)\n",
				"plt.title('Sentiment vs. Number of Comments')\n",
				"plt.xlabel('Sentiment Polarity')\n",
				"plt.ylabel('Number of Comments')\n",
				"\n",
				"plt.tight_layout()\n",
				"plt.show()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Building a Simple Recommendation System\n",
				"\n",
				"Let's build a simple content-based recommendation system for Hacker News stories based on title similarity."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"from sklearn.feature_extraction.text import TfidfVectorizer\n",
				"from sklearn.metrics.pairwise import cosine_similarity\n",
				"import numpy as np\n",
				"\n",
				"# Get a larger dataset for recommendations\n",
				"all_stories = top_stories + new_stories + best_stories\n",
				"all_story_titles = [story.get('title', '') for story in all_stories if 'title' in story]\n",
				"all_story_ids = [story.get('id') for story in all_stories if 'title' in story]\n",
				"\n",
				"# Create a mapping from ID to index\n",
				"id_to_index = {id: idx for idx, id in enumerate(all_story_ids)}\n",
				"\n",
				"# Convert titles to TF-IDF features\n",
				"vectorizer = TfidfVectorizer(stop_words='english')\n",
				"tfidf_matrix = vectorizer.fit_transform(all_story_titles)\n",
				"\n",
				"# Function to get recommendations based on a story ID\n",
				"def get_story_recommendations(story_id, top_n=5):\n",
				"    \"\"\"\n",
				"    Get content-based recommendations for a story based on title similarity.\n",
				"\n",
				"    :param story_id: The ID of the story to get recommendations for\n",
				"    :param top_n: Number of recommendations to return\n",
				"    :return: List of recommended story dictionaries\n",
				"    \"\"\"\n",
				"    if story_id not in id_to_index:\n",
				"        print(f\"Story ID {story_id} not found in dataset\")\n",
				"        return []\n",
				"\n",
				"    # Get the index of the story\n",
				"    idx = id_to_index[story_id]\n",
				"\n",
				"    # Calculate similarity scores\n",
				"    similarity_scores = cosine_similarity(tfidf_matrix[idx], tfidf_matrix).flatten()\n",
				"\n",
				"    # Get indices of top similar stories (excluding the input story)\n",
				"    similar_indices = similarity_scores.argsort()[::-1][1:top_n+1]\n",
				"\n",
				"    # Get the recommended stories\n",
				"    recommendations = [all_stories[i] for i in similar_indices]\n",
				"\n",
				"    return recommendations\n",
				"\n",
				"# Get recommendations for the top story\n",
				"top_story_id = df_stories.iloc[0]['id']\n",
				"recommended_stories = get_story_recommendations(top_story_id)\n",
				"\n",
				"print(f\"Recommendations for story: {df_stories.iloc[0]['title']}\\n\")\n",
				"for i, story in enumerate(recommended_stories):\n",
				"    print(f\"{i+1}. {story.get('title')} (Score: {story.get('score')})\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Conclusion\n",
				"\n",
				"In this notebook, we've covered how to use the Hacker News API to fetch and analyse data. We've seen how to:\n",
				"\n",
				"1. Fetch stories and comments from the API\n",
				"2. Transform the data into pandas DataFrames\n",
				"3. Analyse and visualise the data to extract insights\n",
				"4. Build a simple recommendation system based on content similarity\n",
				"\n",
				"This demonstrates how APIs can be powerful tools for data scientists, allowing you to access rich, real-time data from the web for analysis and modeling."
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Further Explorations\n",
				"\n",
				"Here are some additional things you could explore with the Hacker News API:\n",
				"\n",
				"1. Analyse how quickly stories rise and fall on the front page\n",
				"2. Build a classifier to predict whether a story will get high engagement\n",
				"3. Analyse comment threads and conversation patterns\n",
				"4. Track specific domains or topics over time\n",
				"5. Build a real-time dashboard to monitor Hacker News activity\n",
				"\n",
				"Remember to always be respectful of the API by implementing rate limiting and caching where appropriate."
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Python 3",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.11.11"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 2
}
